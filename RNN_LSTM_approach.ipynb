{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIHTRpuBt7Ef8lb91O4cX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShivaniGawande/DLFinalProject_Fall22/blob/main/RNN_LSTM_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHop7-_0sLgI",
        "outputId": "63a78a57-afee-4688-cf42-acc07ed6843c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import drive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import gensim\n",
        "import re, string, unicodedata\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import text, sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU, SpatialDropout1D\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from keras.preprocessing.text import Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading data from google drive\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#train_file_id = '15NNft0C1CKlKDzjUKbAgibbXTFe8NFgG'\n",
        "test_file_id = '1J4K-z82tEr4jkwG1Hl1t-eigpcivqt3k' \n",
        "'''\n",
        "download = drive.CreateFile({'id': train_file_id})\n",
        "download.GetContentFile('train.csv')\n",
        "train_data  = pd.read_csv(\"train.csv\")\n",
        "train_data.head()\n",
        "'''\n",
        "url = \"https://raw.githubusercontent.com/EducationalTestingService/sarcasm/master/twitter/sarcasm_detection_shared_task_twitter_training.jsonl\"\n",
        "url2 ='https://raw.githubusercontent.com/wjq-learning/MSTI/main/datasets/Textual%20target%20labels/train'\n",
        "df = pd.read_json(url, lines=True)\n",
        "print(df.head())\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12jwCYrWsT_0",
        "outputId": "af09cafb-4c75-4892-cfe0-48af2a2ee70f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     label                                           response  \\\n",
            "0  SARCASM  @USER @USER @USER I don't get this .. obviousl...   \n",
            "1  SARCASM  @USER @USER trying to protest about . Talking ...   \n",
            "2  SARCASM  @USER @USER @USER He makes an insane about of ...   \n",
            "3  SARCASM  @USER @USER Meanwhile Trump won't even release...   \n",
            "4  SARCASM  @USER @USER Pretty Sure the Anti-Lincoln Crowd...   \n",
            "\n",
            "                                             context  \n",
            "0  [A minor child deserves privacy and should be ...  \n",
            "1  [@USER @USER Why is he a loser ? He's just a P...  \n",
            "2  [Donald J . Trump is guilty as charged . The e...  \n",
            "3  [Jamie Raskin tanked Doug Collins . Collins lo...  \n",
            "4  [Man ... y ’ all gone “ both sides ” the apoca...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum() # Checking for NaN values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-tLWI5OsWDD",
        "outputId": "fc7ba0cc-d579-4341-b395-e2e40ec379f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label       0\n",
              "response    0\n",
              "context     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1AHsKh7LsZnV",
        "outputId": "9e8a1aff-7326-464a-b583-841c9998e409"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     label                                           response  \\\n",
              "0  SARCASM  @USER @USER @USER I don't get this .. obviousl...   \n",
              "1  SARCASM  @USER @USER trying to protest about . Talking ...   \n",
              "2  SARCASM  @USER @USER @USER He makes an insane about of ...   \n",
              "3  SARCASM  @USER @USER Meanwhile Trump won't even release...   \n",
              "4  SARCASM  @USER @USER Pretty Sure the Anti-Lincoln Crowd...   \n",
              "\n",
              "                                             context  \n",
              "0  [A minor child deserves privacy and should be ...  \n",
              "1  [@USER @USER Why is he a loser ? He's just a P...  \n",
              "2  [Donald J . Trump is guilty as charged . The e...  \n",
              "3  [Jamie Raskin tanked Doug Collins . Collins lo...  \n",
              "4  [Man ... y ’ all gone “ both sides ” the apoca...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b58f05ba-6369-40ba-baab-5d6632230521\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>response</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SARCASM</td>\n",
              "      <td>@USER @USER @USER I don't get this .. obviousl...</td>\n",
              "      <td>[A minor child deserves privacy and should be ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SARCASM</td>\n",
              "      <td>@USER @USER trying to protest about . Talking ...</td>\n",
              "      <td>[@USER @USER Why is he a loser ? He's just a P...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SARCASM</td>\n",
              "      <td>@USER @USER @USER He makes an insane about of ...</td>\n",
              "      <td>[Donald J . Trump is guilty as charged . The e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SARCASM</td>\n",
              "      <td>@USER @USER Meanwhile Trump won't even release...</td>\n",
              "      <td>[Jamie Raskin tanked Doug Collins . Collins lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SARCASM</td>\n",
              "      <td>@USER @USER Pretty Sure the Anti-Lincoln Crowd...</td>\n",
              "      <td>[Man ... y ’ all gone “ both sides ” the apoca...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b58f05ba-6369-40ba-baab-5d6632230521')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b58f05ba-6369-40ba-baab-5d6632230521 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b58f05ba-6369-40ba-baab-5d6632230521');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_style(\"dark\")\n",
        "sns.countplot(df.label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "0-2FiPtpsbTX",
        "outputId": "79048f40-3633-4fb8-8958-f94d1da4be23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdf6d538130>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx,row in df.iterrows():\n",
        "    row[0] = row[0].replace('rt',' ')\n",
        "    \n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "df2 = df['response'].copy()\n",
        "#df2['context'] = df['context'].copy()\n",
        "tokenizer.fit_on_texts(df2.values)\n",
        "X = tokenizer.texts_to_sequences(df2.values)\n",
        "X = pad_sequences(X)"
      ],
      "metadata": {
        "id": "pUTS3E14HTsS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = pd.get_dummies(df['label']).values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zssrqZj7HTvY",
        "outputId": "125d035d-ac70-4ec4-beca-e26b2fa0d8c5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3350, 61) (3350, 2)\n",
            "(1650, 61) (1650, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(2,activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxqe5MN9Jl7A",
        "outputId": "f0788a44-8e8b-42a4-daab-bae638f36f9e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "history = model.fit(X_train, Y_train, epochs = 25, batch_size=batch_size, verbose = 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znOUk8lhJl-4",
        "outputId": "9e12ce47-ccaa-4c7d-db39-fc8a45dbd165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "105/105 - 33s - loss: 0.6403 - accuracy: 0.6275 - 33s/epoch - 317ms/step\n",
            "Epoch 2/25\n",
            "105/105 - 30s - loss: 0.4947 - accuracy: 0.7678 - 30s/epoch - 289ms/step\n",
            "Epoch 3/25\n",
            "105/105 - 25s - loss: 0.3978 - accuracy: 0.8266 - 25s/epoch - 240ms/step\n",
            "Epoch 4/25\n",
            "105/105 - 25s - loss: 0.3196 - accuracy: 0.8624 - 25s/epoch - 235ms/step\n",
            "Epoch 5/25\n",
            "105/105 - 24s - loss: 0.2535 - accuracy: 0.8928 - 24s/epoch - 232ms/step\n",
            "Epoch 6/25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_size = 1500\n",
        "\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "metadata": {
        "id": "C0agb7iOJmA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "#plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "plt.savefig('model_accuracy.png')\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "#plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "plt.savefig('model_loss.png')"
      ],
      "metadata": {
        "id": "JZv-HkIvJ8ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
        "for x in range(len(X_validate)):\n",
        "    \n",
        "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
        "   \n",
        "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
        "        if np.argmax(Y_validate[x]) == 0:\n",
        "            neg_correct += 1\n",
        "        else:\n",
        "            pos_correct += 1\n",
        "       \n",
        "    if np.argmax(Y_validate[x]) == 0:\n",
        "        neg_cnt += 1\n",
        "    else:\n",
        "        pos_cnt += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"Sarcasm_acc\", pos_correct/pos_cnt*100, \"%\")\n",
        "print(\"Non-Sarcasm_acc\", neg_correct/neg_cnt*100, \"%\")"
      ],
      "metadata": {
        "id": "zPwJ7f-bJ8kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "67Y6jYFfJ8nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jK5iCbf5JmDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a74ixBH4JmGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZAP1E2bHT04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    hidden_size = 1024\n",
        "    num_attention_heads = 16\n",
        "    attention_probs_dropout_prob = 0.5\n",
        "    hidden_dropout_prob = 0.5"
      ],
      "metadata": {
        "id": "NVkhePifsdFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = [transforms.ToTensor()]\n",
        "class CustomDataSet(torch.utils.data.TensorDataset):\n",
        "    def __init__(self, params, x, x_flair, x_c, y, img_id, s_idx, e_idx):\n",
        "        self.params = params\n",
        "        self.x = x\n",
        "        # print(self.x)\n",
        "        self.x_flair = x_flair\n",
        "\n",
        "        self.x_c = x_c\n",
        "\n",
        "\n",
        "        self.y = y\n",
        "        # print(self.y)\n",
        "        self.img_id = img_id\n",
        "        self.s_idx = s_idx\n",
        "        self.e_idx = e_idx\n",
        "        self.grid_num = 7\n",
        "        self.image_size = 608\n",
        "        self.mean = (123,117,104)\n",
        "        self.num_of_samples = e_idx - s_idx\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_of_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[self.s_idx + idx]\n",
        "        x_flair = self.x_flair[self.s_idx + idx]\n",
        "        y = self.y[self.s_idx + idx]\n",
        "        x_c = self.x_c[self.s_idx + idx]\n",
        "        img_id = self.img_id[self.s_idx + idx]\n",
        "        \n",
        "        img_path = os.path.join(self.params.image_obj_features_dir, img_id + '.jpg')\n",
        "        img = cv2.imread(img_path)\n",
        "        ori_img = img\n",
        "        h, w, _ = ori_img.shape\n",
        "        size = [h, w]\n",
        "\n",
        "        img = cv2.imread(img_path)\n",
        "        img = self.BGR2RGB(img)  # because pytorch pretrained model use RGB\n",
        "        img = self.subMean(img, self.mean)\n",
        "        img = cv2.resize(img, (self.image_size, self.image_size))\n",
        "        for t in transform:\n",
        "            img = t(img)\n",
        "        img = np.array(img)\n",
        "\n",
        "        box_path = os.path.join(self.params.image_obj_boxes_dir, img_id + '.txt')\n",
        "        box_f = open(box_path, 'r', encoding='utf-8')\n",
        "        bboxes = []\n",
        "\n",
        "        for line in box_f.readlines():\n",
        "            splited = line.strip().split()\n",
        "            # print(splited)\n",
        "            num_boxes = len(splited) // 5\n",
        "            # print(num_boxes)\n",
        "            for i in range(num_boxes):\n",
        "                x1 = float(splited[0 + 5 * i]) / w * self.image_size\n",
        "                y1 = float(splited[1 + 5 * i]) / h * self.image_size\n",
        "                x2 = float(splited[2 + 5 * i]) / w * self.image_size\n",
        "                y2 = float(splited[3 + 5 * i]) / h * self.image_size\n",
        "                c = int(splited[4 + 5 * i])\n",
        "                # print(i, x1, y1, x2, y2, c)\n",
        "                # if c != 0:\n",
        "                #     print(x1, y1, x2, y2, c)\n",
        "                bboxes.append([x1, y1, x2, y2, 0])\n",
        "\n",
        "        out_bboxes = np.array(bboxes, dtype=np.float)\n",
        "        out_bboxes1 = np.zeros([60, 5])\n",
        "        out_bboxes1[:min(out_bboxes.shape[0], 60)] = out_bboxes[:min(out_bboxes.shape[0], 60)]\n",
        "        # print(images.shape, out_bboxes1.shape)\n",
        "        # print(ori_img.shape)\n",
        "        # print\n",
        "        # print(bboxes)\n",
        "        return x, x_flair, y, x_c, img, out_bboxes1, size, bboxes\n",
        "\n",
        "    def collate(self, batch):\n",
        "        x = np.array([x[0] for x in batch])\n",
        "        x_flair = [x[1] for x in batch]\n",
        "        y = np.array([x[2] for x in batch])\n",
        "        x_c = np.array([x[3] for x in batch])\n",
        "        img = np.array([x[4] for x in batch])\n",
        "        target = np.array([x[5] for x in batch])\n",
        "        size = np.array([x[6] for x in batch])\n",
        "        # print(np.array([z[7] for z in batch]).shape)\n",
        "        # ori_img = np.array([x[7] for x in batch])\n",
        "        bboxes = np.array([x[7] for x in batch])\n",
        "\n",
        "        bool_mask = y == 0\n",
        "        mask = 1 - bool_mask.astype(np.int)\n",
        "\n",
        "        # index of first 0 in each row, if no zero then idx = -1\n",
        "        zero_indices = np.where(bool_mask.any(1), bool_mask.argmax(1), -1).astype(np.int)\n",
        "        # print(zero_indices)\n",
        "        input_len = np.zeros(len(batch))\n",
        "        for i in range(len(batch)):\n",
        "            if zero_indices[i] == -1:\n",
        "                input_len[i] = len(x[i])\n",
        "            else:\n",
        "                input_len[i] = zero_indices[i]\n",
        "        sorted_input_arg = np.argsort(-input_len)\n",
        "\n",
        "        x = x[sorted_input_arg]\n",
        "        x_flair = sorted(x_flair, key=lambda i: len(i), reverse=True)\n",
        "\n",
        "        y = y[sorted_input_arg]\n",
        "        # print(y)\n",
        "        mask = mask[sorted_input_arg]\n",
        "        # mask_object = mask_object[sorted_input_arg]\n",
        "        x_c = x_c[sorted_input_arg]\n",
        "        img = img[sorted_input_arg]\n",
        "        target = target[sorted_input_arg]\n",
        "        size = size[sorted_input_arg]\n",
        "        # ori_img = ori_img[sorted_input_arg]\n",
        "\n",
        "        input_len = input_len[sorted_input_arg]\n",
        "        # img_id = img_id[sorted_input_arg]\n",
        "\n",
        "        max_seq_len = int(input_len[0])\n",
        "\n",
        "        # trunc_x = np.zeros((len(batch), max_seq_len))\n",
        "        trunc_x = np.zeros((len(batch), max_seq_len))\n",
        "        trunc_x_flair = []\n",
        "\n",
        "        trunc_y = np.zeros((len(batch), max_seq_len))\n",
        "        trunc_x_c = np.zeros((len(batch), max_seq_len, self.params.word_maxlen))\n",
        "\n",
        "        trunc_mask = np.zeros((len(batch), max_seq_len))\n",
        "        # print(len(batch))\n",
        "        for i in range(len(batch)):\n",
        "            # print('max_seq_len:', max_seq_len)\n",
        "            # print('x_len:', len(x[0]))\n",
        "            # print('y:', y)\n",
        "            trunc_x_flair.append(x_flair[i])\n",
        "            trunc_x[i] = x[i, :max_seq_len]\n",
        "\n",
        "            trunc_y[i] = y[i, :max_seq_len]\n",
        "            trunc_mask[i] = mask[i, :max_seq_len]\n",
        "            trunc_x_c[i] = x_c[i, :max_seq_len, :]\n",
        "\n",
        "        return to_tensor(trunc_x).long(), trunc_x_flair, to_tensor(trunc_y).long(), to_tensor(trunc_mask).long(), \\\n",
        "               to_tensor(trunc_x_c).long(), to_tensor(input_len).int(), to_tensor(img), to_tensor(target), \\\n",
        "               to_tensor(size), bboxes\n",
        "\n",
        "    def encoder(self, boxes, labels):\n",
        "        '''\n",
        "        boxes (tensor) [[x1,y1,x2,y2],[]]\n",
        "        labels (tensor) [...]\n",
        "        return 7x7x30\n",
        "        '''\n",
        "        target = torch.zeros((self.grid_num, self.grid_num, 11))\n",
        "        cell_size = 1. / self.grid_num\n",
        "        wh = boxes[:, 2:] - boxes[:, :2]\n",
        "        cxcy = (boxes[:, 2:] + boxes[:, :2]) / 2\n",
        "        for i in range(cxcy.size()[0]):\n",
        "            cxcy_sample = cxcy[i]\n",
        "            ij = (cxcy_sample / cell_size).ceil() - 1  #\n",
        "            target[int(ij[1]), int(ij[0]), 4] = 1\n",
        "            target[int(ij[1]), int(ij[0]), 9] = 1\n",
        "            target[int(ij[1]), int(ij[0]), int(labels[i]) + 9] = 1\n",
        "            xy = ij * cell_size\n",
        "            delta_xy = (cxcy_sample - xy) / cell_size\n",
        "            target[int(ij[1]), int(ij[0]), 2:4] = wh[i]\n",
        "            target[int(ij[1]), int(ij[0]), :2] = delta_xy\n",
        "            target[int(ij[1]), int(ij[0]), 7:9] = wh[i]\n",
        "            target[int(ij[1]), int(ij[0]), 5:7] = delta_xy\n",
        "        return target\n",
        "\n",
        "    def BGR2RGB(self, img):\n",
        "        return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    def BGR2HSV(self, img):\n",
        "        return cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    def HSV2BGR(self, img):\n",
        "        return cv2.cvtColor(img, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    def RandomBrightness(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            hsv = self.BGR2HSV(bgr)\n",
        "            h, s, v = cv2.split(hsv)\n",
        "            adjust = random.choice([0.5, 1.5])\n",
        "            v = v * adjust\n",
        "            v = np.clip(v, 0, 255).astype(hsv.dtype)\n",
        "            hsv = cv2.merge((h, s, v))\n",
        "            bgr = self.HSV2BGR(hsv)\n",
        "        return bgr\n",
        "\n",
        "    def RandomSaturation(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            hsv = self.BGR2HSV(bgr)\n",
        "            h, s, v = cv2.split(hsv)\n",
        "            adjust = random.choice([0.5, 1.5])\n",
        "            s = s * adjust\n",
        "            s = np.clip(s, 0, 255).astype(hsv.dtype)\n",
        "            hsv = cv2.merge((h, s, v))\n",
        "            bgr = self.HSV2BGR(hsv)\n",
        "        return bgr\n",
        "\n",
        "    def RandomHue(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            hsv = self.BGR2HSV(bgr)\n",
        "            h, s, v = cv2.split(hsv)\n",
        "            adjust = random.choice([0.5, 1.5])\n",
        "            h = h * adjust\n",
        "            h = np.clip(h, 0, 255).astype(hsv.dtype)\n",
        "            hsv = cv2.merge((h, s, v))\n",
        "            bgr = self.HSV2BGR(hsv)\n",
        "        return bgr\n",
        "\n",
        "    def randomBlur(self, bgr):\n",
        "        if random.random() < 0.5:\n",
        "            bgr = cv2.blur(bgr, (5, 5))\n",
        "        return bgr\n",
        "\n",
        "    def randomShift(self, bgr, boxes, labels):\n",
        "\n",
        "        center = (boxes[:, 2:] + boxes[:, :2]) / 2\n",
        "        if random.random() < 0.5:\n",
        "            height, width, c = bgr.shape\n",
        "            after_shfit_image = np.zeros((height, width, c), dtype=bgr.dtype)\n",
        "            after_shfit_image[:, :, :] = (104, 117, 123)  # bgr\n",
        "            shift_x = random.uniform(-width * 0.2, width * 0.2)\n",
        "            shift_y = random.uniform(-height * 0.2, height * 0.2)\n",
        "\n",
        "            if shift_x >= 0 and shift_y >= 0:\n",
        "                after_shfit_image[int(shift_y):, int(shift_x):, :] = bgr[:height - int(shift_y), :width - int(shift_x),\n",
        "                                                                     :]\n",
        "            elif shift_x >= 0 and shift_y < 0:\n",
        "                after_shfit_image[:height + int(shift_y), int(shift_x):, :] = bgr[-int(shift_y):, :width - int(shift_x),\n",
        "                                                                              :]\n",
        "            elif shift_x < 0 and shift_y >= 0:\n",
        "                after_shfit_image[int(shift_y):, :width + int(shift_x), :] = bgr[:height - int(shift_y), -int(shift_x):,\n",
        "                                                                             :]\n",
        "            elif shift_x < 0 and shift_y < 0:\n",
        "                after_shfit_image[:height + int(shift_y), :width + int(shift_x), :] = bgr[-int(shift_y):,\n",
        "                                                                                      -int(shift_x):, :]\n",
        "\n",
        "            shift_xy = torch.FloatTensor([[int(shift_x), int(shift_y)]]).expand_as(center)\n",
        "            center = center + shift_xy\n",
        "            mask1 = (center[:, 0] > 0) & (center[:, 0] < width)\n",
        "            mask2 = (center[:, 1] > 0) & (center[:, 1] < height)\n",
        "            mask = (mask1 & mask2).view(-1, 1)\n",
        "            boxes_in = boxes[mask.expand_as(boxes)].view(-1, 4)\n",
        "            if len(boxes_in) == 0:\n",
        "                return bgr, boxes, labels\n",
        "            box_shift = torch.FloatTensor([[int(shift_x), int(shift_y), int(shift_x), int(shift_y)]]).expand_as(\n",
        "                boxes_in)\n",
        "            boxes_in = boxes_in + box_shift\n",
        "            labels_in = labels[mask.view(-1)]\n",
        "            return after_shfit_image, boxes_in, labels_in\n",
        "        return bgr, boxes, labels\n",
        "\n",
        "    def randomScale(self, bgr, boxes):\n",
        "\n",
        "        if random.random() < 0.5:\n",
        "            scale = random.uniform(0.8, 1.2)\n",
        "            height, width, c = bgr.shape\n",
        "            bgr = cv2.resize(bgr, (int(width * scale), height))\n",
        "            scale_tensor = torch.FloatTensor([[scale, 1, scale, 1]]).expand_as(boxes)\n",
        "            boxes = boxes * scale_tensor\n",
        "            return bgr, boxes\n",
        "        return bgr, boxes\n",
        "\n",
        "    def randomCrop(self, bgr, boxes, labels):\n",
        "        if random.random() < 0.5:\n",
        "            center = (boxes[:, 2:] + boxes[:, :2]) / 2\n",
        "            height, width, c = bgr.shape\n",
        "            h = random.uniform(0.6 * height, height)\n",
        "            w = random.uniform(0.6 * width, width)\n",
        "            x = random.uniform(0, width - w)\n",
        "            y = random.uniform(0, height - h)\n",
        "            x, y, h, w = int(x), int(y), int(h), int(w)\n",
        "\n",
        "            center = center - torch.FloatTensor([[x, y]]).expand_as(center)\n",
        "            mask1 = (center[:, 0] > 0) & (center[:, 0] < w)\n",
        "            mask2 = (center[:, 1] > 0) & (center[:, 1] < h)\n",
        "            mask = (mask1 & mask2).view(-1, 1)\n",
        "\n",
        "            boxes_in = boxes[mask.expand_as(boxes)].view(-1, 4)\n",
        "            if (len(boxes_in) == 0):\n",
        "                return bgr, boxes, labels\n",
        "            box_shift = torch.FloatTensor([[x, y, x, y]]).expand_as(boxes_in)\n",
        "\n",
        "            boxes_in = boxes_in - box_shift\n",
        "            boxes_in[:, 0] = boxes_in[:, 0].clamp_(min=0, max=w)\n",
        "            boxes_in[:, 2] = boxes_in[:, 2].clamp_(min=0, max=w)\n",
        "            boxes_in[:, 1] = boxes_in[:, 1].clamp_(min=0, max=h)\n",
        "            boxes_in[:, 3] = boxes_in[:, 3].clamp_(min=0, max=h)\n",
        "\n",
        "            labels_in = labels[mask.view(-1)]\n",
        "            img_croped = bgr[y:y + h, x:x + w, :]\n",
        "            return img_croped, boxes_in, labels_in\n",
        "        return bgr, boxes, labels\n",
        "\n",
        "    def subMean(self, bgr, mean):\n",
        "        mean = np.array(mean, dtype=np.float32)\n",
        "        bgr = bgr - mean\n",
        "        return bgr\n",
        "\n",
        "    def random_flip(self, im, boxes):\n",
        "        if random.random() < 0.5:\n",
        "            im_lr = np.fliplr(im).copy()\n",
        "            h, w, _ = im.shape\n",
        "            xmin = w - boxes[:, 2]\n",
        "            xmax = w - boxes[:, 0]\n",
        "            boxes[:, 0] = xmin\n",
        "            boxes[:, 2] = xmax\n",
        "            return im_lr, boxes\n",
        "        return im, boxes\n",
        "\n",
        "    def random_bright(self, im, delta=16):\n",
        "        alpha = random.random()\n",
        "        if alpha > 0.3:\n",
        "            im = im * alpha + random.randrange(-delta, delta)\n",
        "            im = im.clip(min=0, max=255).astype(np.uint8)\n",
        "        return im\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, params):\n",
        "        '''\n",
        "        self.x : sentence encoding with padding at word level\n",
        "        self.x_c : sentence encoding with padding at character level\n",
        "        self.x_img : image features corresponding to the sentences\n",
        "        self.y : label corresponding to the words in the sentences\n",
        "        :param params:\n",
        "        '''\n",
        "        self.params = params\n",
        "\n",
        "        self.id_to_vocb, \\\n",
        "            self.sentences, self.datasplit, \\\n",
        "            self.x, self.x_flair, self.x_c, self.y, \\\n",
        "            self.num_sentence, self.vocb, \\\n",
        "            self.vocb_char, self.labelVoc ,self.img_id\\\n",
        "            = self.load_data()\n",
        "\n",
        "        kwargs = {'num_workers': 8, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
        "\n",
        "        dataset_train = CustomDataSet(params, self.x,  self.x_flair, self.x_c, self.y, self.img_id, self.datasplit[0], self.datasplit[1])\n",
        "        self.train_data_loader = torch.utils.data.DataLoader(dataset_train,\n",
        "                                                             batch_size=self.params.batch_size,\n",
        "                                                             collate_fn=dataset_train.collate,\n",
        "                                                             shuffle=True, **kwargs)\n",
        "\n",
        "        dataset_val = CustomDataSet(params, self.x, self.x_flair, self.x_c, self.y,  self.img_id, self.datasplit[1], self.datasplit[2])\n",
        "        self.val_data_loader = torch.utils.data.DataLoader(dataset_val,\n",
        "                                                           batch_size=1,\n",
        "                                                           collate_fn=dataset_val.collate,\n",
        "                                                           shuffle=False, **kwargs)\n",
        "        dataset_test = CustomDataSet(params, self.x,  self.x_flair, self.x_c, self.y, self.img_id, self.datasplit[2], self.datasplit[3])\n",
        "        self.test_data_loader = torch.utils.data.DataLoader(dataset_test,\n",
        "                                                            batch_size=1,\n",
        "                                                            collate_fn=dataset_test.collate,\n",
        "                                                            shuffle=False, **kwargs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "        print('calculating vocabulary...')\n",
        "\n",
        "        datasplit, sentences, sent_maxlen, word_maxlen, num_sentence,  img_id = self.load_sentence(\n",
        "            'IMGID', self.params.split_file, 'train', 'val', 'test')\n",
        "\n",
        "        id_to_vocb, vocb, vocb_inv, vocb_char, vocb_inv_char, labelVoc, labelVoc_inv = self.vocab_bulid(sentences)\n",
        "\n",
        "        # word_matrix = self.load_word_matrix(vocb, size=self.params.embedding_dimension)\n",
        "\n",
        "        x, x_flair, x_c, y = self.pad_sequence(sentences, vocb, vocb_char, labelVoc,\n",
        "                                               word_maxlen=self.params.word_maxlen, sent_maxlen=sent_maxlen)\n",
        "\n",
        "        return [id_to_vocb, sentences, datasplit, x, x_flair, x_c, y, num_sentence,  vocb, vocb_char,\n",
        "                labelVoc, img_id]\n",
        "\n",
        "    def load_sentence(self, IMAGEID, tweet_data_dir, train_name, val_name, test_name):\n",
        "        \"\"\"\n",
        "        read the word from doc, and build sentence. every line contain a word and it's tag\n",
        "        every sentence is split with a empty line. every sentence begain with an \"IMGID:num\"\n",
        "        \"\"\"\n",
        "        # IMAGEID='IMGID'\n",
        "        img_id = []\n",
        "        sentences = []\n",
        "        sentence = []\n",
        "        sent_maxlen = 0\n",
        "        word_maxlen = 0\n",
        "        obj_features = []\n",
        "        # img_feature = []\n",
        "        datasplit = []\n",
        "        # mask_object = []\n",
        "\n",
        "        for fname in (train_name, val_name, test_name):\n",
        "            datasplit.append(len(img_id))\n",
        "            with open(os.path.join(tweet_data_dir, fname), 'r', encoding='utf-8') as file:\n",
        "                last_line = ''\n",
        "                for line in file:\n",
        "                    line = line.rstrip()\n",
        "                    if line == '':\n",
        "                        sent_maxlen = max(sent_maxlen, len(sentence))\n",
        "                        sentences.append(sentence)\n",
        "                        sentence = []\n",
        "                    else:\n",
        "                        if IMAGEID in line:\n",
        "                            num = line[6:]\n",
        "                            img_id.append(num)\n",
        "                            if last_line != '':\n",
        "                                print(num)\n",
        "                        else:\n",
        "                            if len(line.split()) == 1:\n",
        "                                print(line)\n",
        "                            sentence.append(line.split())\n",
        "                            word_maxlen = max(word_maxlen, len(str(line.split()[0])))\n",
        "                    last_line = line\n",
        "\n",
        "        # sentences.append(sentence)\n",
        "        datasplit.append(len(img_id))\n",
        "        num_sentence = len(sentences)\n",
        "\n",
        "        print(\"datasplit\", datasplit)\n",
        "        print(sentences[len(sentences) - 2])\n",
        "        print(sentences[0])\n",
        "\n",
        "\n",
        "        print('sent_maxlen', sent_maxlen)\n",
        "        print('word_maxlen', word_maxlen)\n",
        "        print('number sentence', len(sentences))\n",
        "        print('number image', len(img_id))\n",
        "\n",
        "        return [datasplit, sentences, sent_maxlen, word_maxlen, num_sentence, img_id]\n",
        "\n",
        "    def vocab_bulid(self, sentences):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            sentences list,\n",
        "            the element of the list is (word, label) pair.\n",
        "        output:\n",
        "            some dictionaries.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        chars = []\n",
        "        labels = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # print(sentence)\n",
        "            for word_label in sentence:\n",
        "                # print(word_label)\n",
        "                if word_label[1] != 'O' and word_label[1] != 'B-S' and word_label[1] != 'I-S':\n",
        "                    print(sentence)\n",
        "                words.append(word_label[0])\n",
        "                labels.append(word_label[1])\n",
        "                for char in word_label[0]:\n",
        "                    chars.append(char)\n",
        "        word_counts = Counter(words)\n",
        "        vocb_inv = [x[0] for x in word_counts.most_common()]\n",
        "        vocb = {x: i + 1 for i, x in enumerate(vocb_inv)}\n",
        "        vocb['PAD'] = 0\n",
        "        id_to_vocb = {i: x for x, i in vocb.items()}\n",
        "\n",
        "        char_counts = Counter(chars)\n",
        "        vocb_inv_char = [x[0] for x in char_counts.most_common()]\n",
        "        vocb_char = {x: i + 1 for i, x in enumerate(vocb_inv_char)}\n",
        "\n",
        "        labels_counts = Counter(labels)\n",
        "        print('labels_counts', len(labels_counts))\n",
        "        print(labels_counts)\n",
        "        labelVoc_inv, labelVoc = self.label_index(labels_counts)\n",
        "        print('labelVoc', labelVoc)\n",
        "\n",
        "        return [id_to_vocb, vocb, vocb_inv, vocb_char, vocb_inv_char, labelVoc, labelVoc_inv]\n",
        "\n",
        "    @staticmethod\n",
        "    def label_index(labels_counts):\n",
        "        \"\"\"\n",
        "           the input is the output of Counter. This function defines the (label, index) pair,\n",
        "           and it cast our datasets label to the definition (label, index) pair.\n",
        "        \"\"\"\n",
        "\n",
        "        num_labels = len(labels_counts)\n",
        "        labelVoc_inv = [x[0] for x in labels_counts.most_common()]\n",
        "\n",
        "        labelVoc = {'0': 0,\n",
        "                    'B-S': 1, 'I-S': 2,\n",
        "                    'O': 3}\n",
        "        if len(labelVoc) < num_labels:\n",
        "            for key, value in labels_counts.items():\n",
        "                if not labelVoc.has_key(key):\n",
        "                    labelVoc.setdefault(key, len(labelVoc))\n",
        "        return labelVoc_inv, labelVoc\n",
        "\n",
        "    @staticmethod\n",
        "    def pad_sequences(y, sent_maxlen):\n",
        "        padded = np.zeros((len(y), sent_maxlen))\n",
        "        for i, each in enumerate(y):\n",
        "            trunc_len = min(sent_maxlen, len(each))\n",
        "            padded[i, :trunc_len] = each[:trunc_len]\n",
        "        return padded.astype(np.int32)\n",
        "\n",
        "    def pad_sequence(self, sentences, vocabulary, vocabulary_char, labelVoc, word_maxlen=30,\n",
        "                     sent_maxlen=35):\n",
        "        \"\"\"\n",
        "            This function is used to pad the word into the same length, the word length is set to 30.\n",
        "            Moreover, it also pad each sentence into the same length, the length is set to 35.\n",
        "        \"\"\"\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        # tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "        print(tokenizer)\n",
        "\n",
        "        x = []\n",
        "        x_flair = []\n",
        "        y = []\n",
        "        for sentence in sentences:\n",
        "            w_id = []\n",
        "            y_id = []\n",
        "            st = Sentence()\n",
        "            for idx, word_label in enumerate(sentence):\n",
        "                try:\n",
        "                    w_id.append(tokenizer.vocab[word_label[0].lower()])\n",
        "                except Exception as e:\n",
        "                    w_id.append(tokenizer.vocab['[MASK]'])\n",
        "                st.add_token(word_label[0])\n",
        "                y_id.append(labelVoc[word_label[1]])\n",
        "\n",
        "            x.append(w_id)\n",
        "            x_flair.append(st)\n",
        "            y.append(y_id)\n",
        "\n",
        "        y = self.pad_sequences(y, sent_maxlen)\n",
        "        x = self.pad_sequences(x, sent_maxlen)\n",
        "\n",
        "        x_c = []\n",
        "        for sentence in sentences:\n",
        "            s_pad = np.zeros([sent_maxlen, word_maxlen], dtype=np.int32)\n",
        "            s_c_pad = []\n",
        "            for word_label in sentence:\n",
        "                w_c = []\n",
        "                char_pad = np.zeros([word_maxlen], dtype=np.int32)\n",
        "                for char in word_label[0]:\n",
        "                    try:\n",
        "                        w_c.append(vocabulary_char[char])\n",
        "                    except:\n",
        "                        w_c.append(0)\n",
        "                if len(w_c) <= word_maxlen:\n",
        "                    char_pad[:len(w_c)] = w_c\n",
        "                else:\n",
        "                    char_pad = w_c[:word_maxlen]\n",
        "\n",
        "                s_c_pad.append(char_pad)\n",
        "\n",
        "            for i in range(len(s_c_pad)):\n",
        "                # Post truncating\n",
        "                if i < sent_maxlen:\n",
        "                    s_pad[i, :len(s_c_pad[i])] = s_c_pad[i]\n",
        "            x_c.append(s_pad)\n",
        "\n",
        "        x_c = np.asarray(x_c)\n",
        "        # x = np.asarray(x)\n",
        "        y = np.asarray(y)\n",
        "        # mask_object = np.asarray(mask_object)\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "        return [x, x_flair, x_c, y]"
      ],
      "metadata": {
        "id": "UbqPmN_CsgmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import numpy as np\n",
        "VOC_CLASSES = (    # always index 0\n",
        "    'aeroplane', 'bicycle', 'bird', 'boat',\n",
        "    'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "    'cow', 'diningtable', 'dog', 'horse',\n",
        "    'motorbike', 'person', 'pottedplant',\n",
        "    'sheep', 'sofa', 'train', 'tvmonitor')\n",
        "Color = [[0, 0, 0],\n",
        "                    [128, 0, 0],\n",
        "                    [0, 128, 0],\n",
        "                    [128, 128, 0],\n",
        "                    [0, 0, 128],\n",
        "                    [128, 0, 128],\n",
        "                    [0, 128, 128],\n",
        "                    [128, 128, 128],\n",
        "                    [64, 0, 0],\n",
        "                    [192, 0, 0],\n",
        "                    [64, 128, 0],\n",
        "                    [192, 128, 0],\n",
        "                    [64, 0, 128],\n",
        "                    [192, 0, 128],\n",
        "                    [64, 128, 128],\n",
        "                    [192, 128, 128],\n",
        "                    [0, 64, 0],\n",
        "                    [128, 64, 0],\n",
        "                    [0, 192, 0],\n",
        "                    [128, 192, 0],\n",
        "                    [0, 64, 128]]\n",
        "\n",
        "def voc_ap(rec,prec,use_07_metric=False):\n",
        "    if use_07_metric:\n",
        "        # 11 point metric\n",
        "        ap = 0.\n",
        "        for t in np.arange(0.,1.1,0.1):\n",
        "            if np.sum(rec >= t) == 0:\n",
        "                p = 0\n",
        "            else:\n",
        "                p = np.max(prec[rec>=t])\n",
        "            ap = ap + p/11.\n",
        "\n",
        "    else:\n",
        "        # correct ap caculation\n",
        "        mrec = np.concatenate(([0.],rec,[1.]))\n",
        "        mpre = np.concatenate(([0.],prec,[0.]))\n",
        "\n",
        "        for i in range(mpre.size -1, 0, -1):\n",
        "            mpre[i-1] = np.maximum(mpre[i-1],mpre[i])\n",
        "\n",
        "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "\n",
        "    return ap\n",
        "\n",
        "def voc_eval(preds,target,VOC_CLASSES=VOC_CLASSES,threshold=0.5,use_07_metric=False,):\n",
        "    '''\n",
        "    preds {'cat':[[image_id,confidence,x1,y1,x2,y2],...],'dog':[[],...]}\n",
        "    target {(image_id,class):[[],]}\n",
        "    '''\n",
        "    aps = []\n",
        "    for i,class_ in enumerate(VOC_CLASSES):\n",
        "        pred = preds[class_] #[[image_id,confidence,x1,y1,x2,y2],...]\n",
        "        if len(pred) == 0:\n",
        "            ap = -1\n",
        "            print('---class {} ap {}---'.format(class_,ap))\n",
        "            aps += [ap]\n",
        "            break\n",
        "        #print(pred)\n",
        "        image_ids = [x[0] for x in pred]\n",
        "        confidence = np.array([float(x[1]) for x in pred])\n",
        "        BB = np.array([x[2:] for x in pred])\n",
        "        # sort by confidence\n",
        "        sorted_ind = np.argsort(-confidence)\n",
        "        sorted_scores = np.sort(-confidence)\n",
        "        BB = BB[sorted_ind, :]\n",
        "        image_ids = [image_ids[x] for x in sorted_ind]\n",
        "\n",
        "        # go down dets and mark TPs and FPs\n",
        "        npos = 0.\n",
        "        for (key1,key2) in target:\n",
        "            if key2 == class_:\n",
        "                npos += len(target[(key1,key2)])\n",
        "        nd = len(image_ids)\n",
        "        tp = np.zeros(nd)\n",
        "        fp = np.zeros(nd)\n",
        "        for d,image_id in enumerate(image_ids):\n",
        "            bb = BB[d]\n",
        "            if (image_id,class_) in target:\n",
        "                BBGT = target[(image_id,class_)] #[[],]\n",
        "                for bbgt in BBGT:\n",
        "                    # compute overlaps\n",
        "                    # intersection\n",
        "                    ixmin = np.maximum(bbgt[0], bb[0])\n",
        "                    iymin = np.maximum(bbgt[1], bb[1])\n",
        "                    ixmax = np.minimum(bbgt[2], bb[2])\n",
        "                    iymax = np.minimum(bbgt[3], bb[3])\n",
        "                    iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
        "                    ih = np.maximum(iymax - iymin + 1., 0.)\n",
        "                    inters = iw * ih\n",
        "\n",
        "                    union = (bb[2]-bb[0]+1.)*(bb[3]-bb[1]+1.) + (bbgt[2]-bbgt[0]+1.)*(bbgt[3]-bbgt[1]+1.) - inters\n",
        "                    if union == 0:\n",
        "                        print(bb,bbgt)\n",
        "                    \n",
        "                    overlaps = inters/union\n",
        "                    if overlaps > threshold:\n",
        "                        tp[d] = 1\n",
        "                        BBGT.remove(bbgt)\n",
        "                        if len(BBGT) == 0:\n",
        "                            del target[(image_id,class_)]\n",
        "                        break\n",
        "                fp[d] = 1-tp[d]\n",
        "            else:\n",
        "                fp[d] = 1\n",
        "        fp = np.cumsum(fp)\n",
        "        tp = np.cumsum(tp)\n",
        "        rec = tp/float(npos)\n",
        "        prec = tp/np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
        "\n",
        "        ap = voc_ap(rec, prec, use_07_metric)\n",
        "        print('---class {} ap {}---'.format(class_,ap))\n",
        "        aps += [ap]\n",
        "    print('---map {}---'.format(np.mean(aps)))\n",
        "\n",
        "def test_eval():\n",
        "    preds = {'cat':[['image01',0.9,20,20,40,40],['image01',0.8,20,20,50,50],['image02',0.8,30,30,50,50]],'dog':[['image01',0.78,60,60,90,90]]}\n",
        "    target = {('image01','cat'):[[20,20,41,41]],('image01','dog'):[[60,60,91,91]],('image02','cat'):[[30,30,51,51]]}\n",
        "    voc_eval(preds,target,VOC_CLASSES=['cat','dog'])"
      ],
      "metadata": {
        "id": "Beg8aK5DsiuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from util import *\n",
        "from tqdm import tqdm\n",
        "from torchcrf import CRF\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from predict import decoder\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tool.tv_reference.utils import collate_fn as val_collate\n",
        "from tool.tv_reference.coco_utils import convert_to_coco_api\n",
        "from tool.tv_reference.coco_eval import CocoEvaluator\n",
        "from tool.utils import post_processing\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "VOC_CLASSES = (    # always index 0\n",
        "    'target')\n",
        "\n",
        "\n",
        "def voc_ap(rec, prec, use_07_metric=False):\n",
        "    if use_07_metric:\n",
        "        # 11 point metric\n",
        "        ap = 0.\n",
        "        for t in np.arange(0., 1.1, 0.1):\n",
        "            if np.sum(rec >= t) == 0:\n",
        "                p = 0\n",
        "            else:\n",
        "                p = np.max(prec[rec >= t])\n",
        "            ap = ap + p / 11.\n",
        "\n",
        "    else:\n",
        "        # correct ap caculation\n",
        "        mrec = np.concatenate(([0.], rec, [1.]))\n",
        "        mpre = np.concatenate(([0.], prec, [0.]))\n",
        "\n",
        "        for i in range(mpre.size - 1, 0, -1):\n",
        "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "\n",
        "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "\n",
        "    return ap\n",
        "\n",
        "\n",
        "def voc_eval(pred, target, threshold=0.5, use_07_metric=False, ):\n",
        "    '''\n",
        "    preds {'cat':[[image_id,confidence,x1,y1,x2,y2],...],'dog':[[],...]}\n",
        "    target {(image_id,class):[[],]}\n",
        "    '''\n",
        "    # print(pred)\n",
        "    if len(pred) == 0:\n",
        "        return -1\n",
        "    image_ids = [x[0] for x in pred]\n",
        "    confidence = np.array([float(x[3]) for x in pred])\n",
        "    BB = np.array([[x[1][0], x[1][1], x[2][0], x[2][1]] for x in pred])\n",
        "    # print(image_ids)\n",
        "    # print(BB)\n",
        "    # print(confidence)\n",
        "    # sort by confidence\n",
        "    sorted_ind = np.argsort(-confidence)\n",
        "    # sorted_scores = np.sort(-confidence)\n",
        "    BB = BB[sorted_ind, :]\n",
        "    # print(BB)\n",
        "    image_ids = [image_ids[x] for x in sorted_ind]\n",
        "    # print(image_ids)\n",
        "    # go down dets and mark TPs and FPs\n",
        "    npos = 0.\n",
        "    for key1 in target:\n",
        "        npos += len(target[key1])\n",
        "    # print(npos)\n",
        "    nd = len(image_ids)\n",
        "    tp = np.zeros(nd)\n",
        "    fp = np.zeros(nd)\n",
        "    for d, image_id in enumerate(image_ids):\n",
        "        bb = BB[d]\n",
        "        if image_id in target:\n",
        "            temp = target[image_id]\n",
        "            # print(temp)\n",
        "            BBGT = [[item[0][0], item[0][1], item[1][0], item[1][1]] for item in temp]\n",
        "            # print(BBGT)\n",
        "            for bbgt in BBGT:\n",
        "                # compute overlaps\n",
        "                # intersection\n",
        "                ixmin = np.maximum(bbgt[0], bb[0])\n",
        "                iymin = np.maximum(bbgt[1], bb[1])\n",
        "                ixmax = np.minimum(bbgt[2], bb[2])\n",
        "                iymax = np.minimum(bbgt[3], bb[3])\n",
        "                iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
        "                ih = np.maximum(iymax - iymin + 1., 0.)\n",
        "                inters = iw * ih\n",
        "\n",
        "                union = (bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) + (bbgt[2] - bbgt[0] + 1.) * (\n",
        "                        bbgt[3] - bbgt[1] + 1.) - inters\n",
        "                if union == 0:\n",
        "                    print(bb, bbgt)\n",
        "\n",
        "                overlaps = inters / union\n",
        "                if overlaps > threshold:\n",
        "                    tp[d] = 1\n",
        "                    BBGT.remove(bbgt)\n",
        "                    if len(BBGT) == 0:\n",
        "                        del target[image_id]\n",
        "                    break\n",
        "            fp[d] = 1 - tp[d]\n",
        "        else:\n",
        "            fp[d] = 1\n",
        "    # print(tp)\n",
        "    # print(fp)\n",
        "    fp = np.cumsum(fp)\n",
        "    tp = np.cumsum(tp)\n",
        "    # print(tp)\n",
        "    # print(fp)\n",
        "    rec = tp / float(npos)\n",
        "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
        "    # print(rec,prec)\n",
        "    ap = voc_ap(rec, prec, use_07_metric)\n",
        "    return ap\n",
        "\n",
        "\n",
        "def draw(ori_img, box, color):\n",
        "    left_up = box[0]\n",
        "    right_bottom = box[1]\n",
        "    # print(left_up)\n",
        "    cv2.rectangle(ori_img, left_up, right_bottom, color, 2)\n",
        "\n",
        "    return ori_img\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    def __init__(self, params, data_loader):\n",
        "        self.params = params\n",
        "        self.data_loader = data_loader\n",
        "\n",
        "    def get_accuracy(self, model, split, crf=None):\n",
        "        if split == 'val':\n",
        "            data_loader = self.data_loader.val_data_loader\n",
        "        else:\n",
        "            data_loader = self.data_loader.test_data_loader\n",
        "\n",
        "        if crf == None:\n",
        "            num_of_tags = len(self.data_loader.labelVoc)\n",
        "            crf = CRF(num_of_tags)\n",
        "            if torch.cuda.is_available():\n",
        "                crf = crf.cuda()\n",
        "\n",
        "        model.eval()\n",
        "        labels_pred = []\n",
        "        labels = []\n",
        "        words = []\n",
        "        sent_lens = []\n",
        "\n",
        "        obj_preds = []\n",
        "        obj_targets = {}\n",
        "        for (i, (x, x_flair, y, mask, x_c, lens, img, target, size, bboxes)) in tqdm(enumerate(data_loader)):\n",
        "            emissions, img_output = model(to_variable(x), x_flair,\n",
        "                                          lens, to_variable(mask),\n",
        "                                          to_variable(x_c), to_variable(img), mode='test')  # seq_len * bs * labels\n",
        "            pre_test_label_index = crf.decode(emissions)  # bs * seq_len\n",
        "            words.append(x)\n",
        "            labels.append(y.cpu().numpy().squeeze(0))\n",
        "            labels_pred.append(pre_test_label_index[0])\n",
        "            sent_lens.append(lens.cpu().numpy()[0])\n",
        "\n",
        "            h = size.squeeze()[0]\n",
        "            w = size.squeeze()[1]\n",
        "\n",
        "            targets_img = []\n",
        "            for j in range(len(bboxes[0])):\n",
        "                box = bboxes[0][j]\n",
        "                x1 = int(box[0] / 608.0 * w)\n",
        "                y1 = int(box[1] / 608.0 * h)\n",
        "                x2 = int(box[2] / 608.0 * w)\n",
        "                y2 = int(box[3] / 608.0 * h)\n",
        "                targets_img.append([(x1, y1), (x2, y2), 1.0])\n",
        "            # print(targets_img)\n",
        "            if targets_img == [[(0, 0), (0, 0), 1.0]]:\n",
        "                targets_img = []\n",
        "\n",
        "            list_features_numpy = []\n",
        "            for feature in img_output:\n",
        "                list_features_numpy.append(feature.data.cpu().numpy())\n",
        "            boxes = post_processing(\n",
        "                img=img,\n",
        "                conf_thresh=self.params.conf_thresh,\n",
        "                n_classes=1,\n",
        "                nms_thresh=self.params.nms_thresh,\n",
        "                list_features_numpy=list_features_numpy\n",
        "            )\n",
        "\n",
        "            preds_img = []\n",
        "            for j in range(len(boxes)):\n",
        "                box = boxes[j]\n",
        "                x1 = int((box[0] - box[2] / 2.0) * w)\n",
        "                y1 = int((box[1] - box[3] / 2.0) * h)\n",
        "                x2 = int((box[0] + box[2] / 2.0) * w)\n",
        "                y2 = int((box[1] + box[3] / 2.0) * h)\n",
        "                preds_img.append([(x1, y1), (x2, y2), box[4]])\n",
        "\n",
        "            # print(i, preds_img)\n",
        "            # print(i, targets_img)\n",
        "\n",
        "            for item in preds_img:\n",
        "                item.insert(0, i)\n",
        "                obj_preds.append(item)\n",
        "\n",
        "            for item in targets_img:\n",
        "                if i not in obj_targets.keys():\n",
        "                    obj_targets[i] = []\n",
        "                obj_targets[i].append(item)\n",
        "\n",
        "        thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
        "        aps = list()\n",
        "        for th in thresholds:\n",
        "            a = obj_preds.copy()\n",
        "            b = obj_targets.copy()\n",
        "            aps.append(voc_eval(a, b, th))\n",
        "\n",
        "        ap = np.mean(aps)\n",
        "        ap50 = aps[0]\n",
        "        ap75 = aps[5]\n",
        "        print(aps)\n",
        "\n",
        "        return self.evaluate(labels_pred, labels, words, sent_lens), ap, ap50, ap75\n",
        "        # return self.evaluate(labels_pred, labels, words, sent_lens), 1\n",
        "\n",
        "    def predict(self, model, split, crf=None):\n",
        "        if split == 'val':\n",
        "            data_loader = self.data_loader.val_data_loader\n",
        "        else:\n",
        "            data_loader = self.data_loader.test_data_loader\n",
        "\n",
        "        if crf == None:\n",
        "            num_of_tags = len(self.data_loader.labelVoc)\n",
        "            crf = CRF(num_of_tags)\n",
        "            if torch.cuda.is_available():\n",
        "                crf = crf.cuda()\n",
        "\n",
        "        model.eval()\n",
        "        labels_pred = []\n",
        "        labels = []\n",
        "        words = []\n",
        "        sent_lens = []\n",
        "\n",
        "        obj_preds = []\n",
        "        obj_targets = {}\n",
        "        for (i, (x, x_flair, y, mask, x_c, lens, img, target, size, bboxes, ori_img, img_id)) in tqdm(enumerate(data_loader)):\n",
        "            emissions, img_output = model(to_variable(x), x_flair,\n",
        "                                          lens, to_variable(mask),\n",
        "                                          to_variable(x_c), to_variable(img), mode='test')  # seq_len * bs * labels\n",
        "            pre_test_label_index = crf.decode(emissions)  # bs * seq_len\n",
        "            words.append(x)\n",
        "            labels.append(y.cpu().numpy().squeeze(0))\n",
        "            labels_pred.append(pre_test_label_index[0])\n",
        "            sent_lens.append(lens.cpu().numpy()[0])\n",
        "\n",
        "            h = size.squeeze()[0]\n",
        "            w = size.squeeze()[1]\n",
        "\n",
        "            targets_img = []\n",
        "            for j in range(len(bboxes[0])):\n",
        "                box = bboxes[0][j]\n",
        "                x1 = int(box[0] / 608.0 * w)\n",
        "                y1 = int(box[1] / 608.0 * h)\n",
        "                x2 = int(box[2] / 608.0 * w)\n",
        "                y2 = int(box[3] / 608.0 * h)\n",
        "                targets_img.append([(x1, y1), (x2, y2), 1.0])\n",
        "            # print(targets_img)\n",
        "            if targets_img == [[(0, 0), (0, 0), 1.0]]:\n",
        "                targets_img = []\n",
        "\n",
        "            list_features_numpy = []\n",
        "            for feature in img_output:\n",
        "                list_features_numpy.append(feature.data.cpu().numpy())\n",
        "            boxes = post_processing(\n",
        "                img=img,\n",
        "                conf_thresh=self.params.conf_thresh,\n",
        "                n_classes=1,\n",
        "                nms_thresh=self.params.nms_thresh,\n",
        "                list_features_numpy=list_features_numpy\n",
        "            )\n",
        "\n",
        "            preds_img = []\n",
        "            for j in range(len(boxes)):\n",
        "                box = boxes[j]\n",
        "                x1 = int((box[0] - box[2] / 2.0) * w)\n",
        "                y1 = int((box[1] - box[3] / 2.0) * h)\n",
        "                x2 = int((box[0] + box[2] / 2.0) * w)\n",
        "                y2 = int((box[1] + box[3] / 2.0) * h)\n",
        "                preds_img.append([(x1, y1), (x2, y2), box[4]])\n",
        "\n",
        "            # if len(preds_img) == 0:\n",
        "            #     preds_img.append([(0, 0), (0, 0), 0.0])\n",
        "            ori_img = ori_img[0]\n",
        "            flag = False\n",
        "\n",
        "            # print(i, preds_img)\n",
        "            # print(i, targets_img)\n",
        "\n",
        "            for item in preds_img:\n",
        "                ori_img = draw(ori_img, item, [0, 0, 255])\n",
        "                item.insert(0, i)\n",
        "                obj_preds.append(item)\n",
        "                flag = True\n",
        "\n",
        "            for item in targets_img:\n",
        "                ori_img = draw(ori_img, item, [255, 0, 0])\n",
        "                if i not in obj_targets.keys():\n",
        "                    obj_targets[i] = []\n",
        "                obj_targets[i].append(item)\n",
        "                flag = True\n",
        "            print(img_id)\n",
        "            if flag:\n",
        "                cv2.imwrite('result/%d.jpg' % img_id, ori_img)\n",
        "\n",
        "        thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
        "        aps = list()\n",
        "        for th in thresholds:\n",
        "            aps.append(voc_eval(obj_preds, obj_targets, th))\n",
        "\n",
        "        ap = np.mean(aps)\n",
        "        ap50 = aps[0]\n",
        "        ap75 = aps[5]\n",
        "\n",
        "        return self.evaluate(labels_pred, labels, words, sent_lens), ap, ap50, ap75\n",
        "\n",
        "    def img_eval(self, target_boxes, target_cls_indexs, target_probs, pred_boxes, pred_cls_indexs, pred_probs, h, w):\n",
        "        preds_img = []\n",
        "        for i, box in enumerate(pred_boxes):\n",
        "            x1 = int(box[0] * w)\n",
        "            x2 = int(box[2] * w)\n",
        "            y1 = int(box[1] * h)\n",
        "            y2 = int(box[3] * h)\n",
        "            prob = float(pred_probs[i])\n",
        "            preds_img.append([(x1, y1), (x2, y2), prob])\n",
        "        targets_img = []\n",
        "        for i, box in enumerate(target_boxes):\n",
        "            x1 = int(box[0] * w)\n",
        "            x2 = int(box[2] * w)\n",
        "            y1 = int(box[1] * h)\n",
        "            y2 = int(box[3] * h)\n",
        "            prob = float(target_probs[i])\n",
        "            targets_img.append([(x1, y1), (x2, y2), prob])\n",
        "        return preds_img, targets_img\n",
        "\n",
        "    def evaluate(self, labels_pred, labels, words, sents_length):\n",
        "        accs = []\n",
        "        ems = []\n",
        "        preds = []\n",
        "        gts = []\n",
        "        # correct_preds, total_correct, total_preds = 0., 0., 0.\n",
        "        TP = 0\n",
        "        TN = 0\n",
        "        FP = 0\n",
        "        FN = 0\n",
        "\n",
        "        for lab, lab_pred, length, word_sent in zip(labels, labels_pred, sents_length, words):\n",
        "            lab = lab[:length]\n",
        "            lab_pred = lab_pred[:length]\n",
        "            # exact_match = [a == b for (a, b) in zip(lab, lab_pred)]\n",
        "            # accs += exact_match\n",
        "            # lab_chunks = set(self.get_chunks(lab, self.data_loader.labelVoc))\n",
        "            # lab_pred_chunks = set(self.get_chunks(lab_pred, self.data_loader.labelVoc))\n",
        "            # correct_preds += len(lab_chunks & lab_pred_chunks)\n",
        "            # total_preds += len(lab_pred_chunks)\n",
        "            # total_correct += len(lab_chunks)\n",
        "            pred = [a == 1 or a == 2 for a in lab_pred]\n",
        "            gt = [a == 1 or a == 2 for a in lab]\n",
        "            exact_match = [a == b for (a, b) in zip(pred, gt)]\n",
        "            accs += exact_match\n",
        "            preds += pred\n",
        "            gts += gt\n",
        "            ems.append(sum(exact_match) // len(exact_match))\n",
        "\n",
        "        # p = correct_preds / total_preds if correct_preds > 0 else 0\n",
        "        # r = correct_preds / total_correct if correct_preds > 0 else 0\n",
        "        # f1 = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
        "        # for a, b in zip(preds, gts):\n",
        "        #     if a == 0 and b == 0:\n",
        "        #         TN += 1\n",
        "        #     if a == 1 and b == 1:\n",
        "        #         TP += 1\n",
        "        #     if a == 0 and b == 1:\n",
        "        #         FP += 1\n",
        "        #     if a == 1 and b == 0:\n",
        "        #         FN += 1\n",
        "\n",
        "        # p = float(TP / (TP + FP))\n",
        "        # r = float(TP / (TP + FN))\n",
        "        # f1 = float(2*TP)/(2*TP + FP + FN)\n",
        "        # print(f1, p, r)\n",
        "        p = precision_score(gts, preds, average='binary')\n",
        "        r = recall_score(gts, preds, average='binary')\n",
        "        f1 = f1_score(gts, preds, average='binary')\n",
        "        acc = np.mean(accs)\n",
        "        EM = np.mean(ems)\n",
        "        return acc, f1, p, r, EM\n",
        "\n",
        "    def get_chunks(self, seq, tags):\n",
        "        \"\"\"\n",
        "        tags:dic{'per':1,....}\n",
        "        Args:\n",
        "            seq: [4, 4, 0, 0, ...] sequence of labels\n",
        "            tags: dict[\"O\"] = 4\n",
        "        Returns:\n",
        "            list of (chunk_type, chunk_start, chunk_end)\n",
        "        Example:\n",
        "            seq = [4, 5, 0, 3]\n",
        "            tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
        "            result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
        "        \"\"\"\n",
        "        default = tags['O']\n",
        "        idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
        "        chunks = []\n",
        "        chunk_type, chunk_start = None, None\n",
        "        for i, tok in enumerate(seq):\n",
        "            # End of a chunk 1\n",
        "            if tok == default and chunk_type is not None:\n",
        "                # Add a chunk.\n",
        "                chunk = (chunk_type, chunk_start, i)\n",
        "                chunks.append(chunk)\n",
        "                chunk_type, chunk_start = None, None\n",
        "\n",
        "            # End of a chunk + start of a chunk!\n",
        "            elif tok != default:\n",
        "                tok_chunk_class, tok_chunk_type = self.get_chunk_type(tok, idx_to_tag)\n",
        "                if chunk_type is None:\n",
        "                    chunk_type, chunk_start = tok_chunk_type, i\n",
        "                elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
        "                    chunk = (chunk_type, chunk_start, i)\n",
        "                    chunks.append(chunk)\n",
        "                    chunk_type, chunk_start = tok_chunk_type, i\n",
        "            else:\n",
        "                pass\n",
        "        # end condition\n",
        "        if chunk_type is not None:\n",
        "            chunk = (chunk_type, chunk_start, len(seq))\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def get_chunk_type(self, tok, idx_to_tag):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            tok: id of token, such as 4\n",
        "            idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
        "        Returns:\n",
        "            tuple: \"B\", \"PER\"\n",
        "        \"\"\"\n",
        "        tag_name = idx_to_tag[tok]\n",
        "        tag_class = tag_name.split('-')[0]\n",
        "        tag_type = tag_name.split('-')[-1]\n",
        "        return tag_class, tag_type\n",
        "\n"
      ],
      "metadata": {
        "id": "d6is2kcatqWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "\n",
        "class BertLayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "        \"\"\"\n",
        "        super(BertLayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.weight * x + self.bias\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output"
      ],
      "metadata": {
        "id": "4c0eDKTItzFh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}